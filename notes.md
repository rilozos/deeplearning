# Notes - Fundamentals of Deep Learning

## Introduction

### Early History

#### Early Neural Networks

- Inspired by biology

#### Expert Systems

- Rigorous programming of many rules

### Deep Learning Revolution

#### Data

- Networks need a lot of information to learn from

#### Computing Power

- GPU have much more cores running in parallel
- GPU's can train on massive amounts of data

#### Machine Learning

- Show model examples with answers on how to classify
- Model able to classify new inputs based on training

#### Deep Learning Models

- Depth and complexity of networks
- Up to billions of parameters
- Many layers in a model
- Important for learning complex rules

#### Natural Language Processing

- Real time translation
- Voice recognition

#### Recommender Systems

- Content curation
- Targeted advertising
- Shopping recommendations

#### Reinforcement Learning

- AI bots beat pro gamers
- Stock trading robots

#### Computer Vision

- Robotics and manufacturing
- Object Detection

## How a Neural Network Trains

#### Activation Functions

- Linear
- ReLU
- Sigmoid

#### Overfitting

- "memorizing the test"
- "cannot classify new datasets
- must be able to generalize to lower MSE (mean squared error)

## CNN's

### Batch normalization
- 0 mean
- constant through time and data
- can normalize the scale, makes training more stable


### Data Augmentation and Deployment

- if training data is small, can augment data to create larger dataset
- examples: inverting an image verticly of a dog, zooming in on dogs


## Pre-Trained Models

## Advanced Architectures

### Natural Language Processing

#### Tokenization

- assigns numbers to words
- can decode the numbers back into words

### Recurrent Neural Networks

### Autoencoders

- can be useful for defect detection

### Reinforcement Learning

- agent and environment
- agent reacts to environment, is rewarded

### NGC test containers